{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd38ff-489d-4f0e-a9fd-429973349d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신발 검색\n",
    "# 블로그가 뜸\n",
    "# 블로그 정해진 개수만큼 가져와서 블로그별로 감정분석\n",
    "# 블로그url을 인자로 감정분석 함수호출\n",
    "# social_media_mentions = [model, positive, negative, neutral] 로 저장 \n",
    "# 위에는 내가 밑에는 지피티가 함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eefd6fb6-c45e-4c14-9ba9-ac6b76562b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2bcdf9-3f47-44f0-bc11-fb73e2ac287d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 푸마 스피드캣 OG 스파르코 블랙 화이트\n",
      "Blog Title: <b>푸마 스피드캣 OG 스파르코 블랙 화이트</b> 리뷰 ft. 코디 사이즈 팁\n",
      "positive_count: 4\n",
      "negative_count: 1\n",
      "neutral_count: 4\n",
      "['푸마 스피드캣 OG 스파르코 블랙 화이트', 1, 0, 0]\n",
      "Blog Title: 이게 돌아올줄이야 <b>푸마 스피드캣</b> LS / <b>OG 스파르코</b> 1월24일... \n",
      "positive_count: 4\n",
      "negative_count: 2\n",
      "neutral_count: 12\n",
      "['푸마 스피드캣 OG 스파르코 블랙 화이트', 1, 0, 1]\n",
      "Blog Title: 화려한 귀환 <b>푸마 스피드캣 OG 스파르코</b> &amp; 퓨마 <b>스피드</b>켓 LS... \n",
      "positive_count: 11\n",
      "negative_count: 2\n",
      "neutral_count: 7\n",
      "['푸마 스피드캣 OG 스파르코 블랙 화이트', 2, 0, 1]\n",
      "Blog Title: 2024트렌드 <b>푸마 스피드캣OG 스파르코 블랙</b> 사이즈... \n",
      "positive_count: 9\n",
      "negative_count: 0\n",
      "neutral_count: 5\n",
      "['푸마 스피드캣 OG 스파르코 블랙 화이트', 3, 0, 1]\n",
      "Blog Title: <b>푸마 스피드캣 OG</b> + <b>스파르코  블랙</b> 리뷰 및 코디 사이즈 팁... \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "#-*- coding: utf-8 -*-\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "# 상위 디렉토리에서 config.py import\n",
    "sys.path.append('..')\n",
    "try:\n",
    "    from config import (\n",
    "        NAVER_SEARCH_CLIENT_ID, \n",
    "        NAVER_SEARCH_CLIENT_SECRET,\n",
    "        NCP_CLIENT_ID,\n",
    "        NCP_CLIENT_SECRET\n",
    "    )\n",
    "    # 검색용 api 인증키\n",
    "    search_client_id = NAVER_SEARCH_CLIENT_ID\n",
    "    search_client_secret = NAVER_SEARCH_CLIENT_SECRET\n",
    "    # 감성분석용 api 인증키\n",
    "    client_id = NCP_CLIENT_ID\n",
    "    client_secret = NCP_CLIENT_SECRET\n",
    "except ImportError:\n",
    "    print(\"config.py 파일이 없습니다. config.example.py를 복사하여 config.py를 만들고 API 키를 입력하세요.\")\n",
    "    print(\"또는 아래 주석을 해제하여 직접 입력하세요.\")\n",
    "    # search_client_id = 'your_naver_search_client_id'\n",
    "    # search_client_secret = 'your_naver_search_client_secret'\n",
    "    # client_id = \"your_ncp_client_id\"\n",
    "    # client_secret = \"your_ncp_client_secret\"\n",
    "    raise\n",
    "\n",
    "nltk.download('punkt', quiet=True)  # 이미 다운로드된 경우 조용히 넘어갑니다.\n",
    "# api url \n",
    "analyze_url = \"https://naveropenapi.apigw.ntruss.com/sentiment-analysis/v1/analyze\" # 감정분석 api url\n",
    "search_url = \"https://openapi.naver.com/v1/search/blog?query=\" # 신발이름으로 검색할거 url\n",
    "start = 1\n",
    "display = 20\n",
    "\n",
    "headers = {\n",
    "    \"X-NCP-APIGW-API-KEY-ID\": client_id,\n",
    "    \"X-NCP-APIGW-API-KEY\": client_secret,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# 네이버 블로그 본문 가져오기\n",
    "def get_naver_blog_content(blog_url):\n",
    "    try:\n",
    "        response = requests.get(blog_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch blog URL {blog_url} with status code {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # iframe URL 추출\n",
    "        iframe = soup.find(\"iframe\", {\"id\": \"mainFrame\"})\n",
    "        if not iframe:\n",
    "            print(f\"No iframe found in blog URL {blog_url}\")\n",
    "            return None\n",
    "        \n",
    "        iframe_url = \"https://blog.naver.com\" + iframe[\"src\"]\n",
    "        response = requests.get(iframe_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch iframe URL {iframe_url} with status code {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        iframe_soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = iframe_soup.find_all(\"p\")\n",
    "\n",
    "        return paragraphs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred while fetching blog content: {e}\")\n",
    "        return None\n",
    "        \n",
    "\n",
    "# 블로그 글 감정 분석하기 \n",
    "def analyze_emot(blog_url):\n",
    "    paragraphs = get_naver_blog_content(blog_url)\n",
    "    if not paragraphs:\n",
    "        print(f\"No paragraphs found in blog URL {blog_url}\")\n",
    "        return None\n",
    "\n",
    "    # 블로그 글의 문장 리스트\n",
    "    sentences = []\n",
    "    for p in paragraphs:\n",
    "        sentences.extend(nltk.sent_tokenize(p.get_text()))\n",
    "\n",
    "    if not sentences:\n",
    "        print(f\"No sentences found in blog URL {blog_url}\")\n",
    "        return None\n",
    "\n",
    "    # 블로그 글을 청크로 나누기 (문장 수 기준)\n",
    "    chunk_size = 10\n",
    "    chunks = [\" \".join(sentences[i:i + chunk_size]) for i in range(0, len(sentences), chunk_size)]\n",
    "\n",
    "    # 각 청크에 대해 감정 분석 수행\n",
    "    sentiments = []\n",
    "    for chunk in chunks:\n",
    "        data = {\n",
    "            \"content\": chunk\n",
    "        }\n",
    "        response = requests.post(analyze_url, data=json.dumps(data), headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            result = json.loads(response.text)\n",
    "            sentiment = result['document']['sentiment']\n",
    "            sentiments.append(sentiment)\n",
    "        else:\n",
    "            print(f\"Error in sentiment analysis API: {response.status_code} - {response.text}\")\n",
    "    \n",
    "    # 전체 글의 감정 판단 (가장 많은 감정으로 글의 감정을 결정)\n",
    "    positive_count = sentiments.count('positive')\n",
    "    negative_count = sentiments.count('negative')\n",
    "    neutral_count = sentiments.count('neutral')\n",
    "    print(f\"positive_count: {positive_count}\\nnegative_count: {negative_count}\\nneutral_count: {neutral_count}\")\n",
    "\n",
    "    if positive_count >= negative_count and positive_count >= neutral_count:\n",
    "        return 'positive'\n",
    "    elif negative_count > positive_count and negative_count >= neutral_count:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# 신발별 감정분석 결과 \n",
    "social_media_mentions = []  # model, positive, negative, neutral\n",
    "\n",
    "# 신발가져오기\n",
    "model_list = pd.read_csv('../dataset/data/RELEASE_PRICE.csv', encoding='utf-8')\n",
    "model_list = model_list['model'][70:]\n",
    "\n",
    "for model in model_list:\n",
    "    print(f\"Model: {model}\")\n",
    "    social_media_mention = [model, 0, 0, 0]\n",
    "    \n",
    "    encText = urllib.parse.quote(model)\n",
    "    url = f\"{search_url}{encText}&start={start}&display={display}\"\n",
    "    \n",
    "    try:\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header(\"X-Naver-Client-Id\", search_client_id)\n",
    "        request.add_header(\"X-Naver-Client-Secret\", search_client_secret)\n",
    "        response = urllib.request.urlopen(request)\n",
    "        rescode = response.getcode()\n",
    "        if rescode == 200:\n",
    "            response_body = response.read()\n",
    "            response_json = json.loads(response_body)\n",
    "        else:\n",
    "            print(f\"Error Code: {rescode}\")\n",
    "            continue\n",
    "        \n",
    "        items = response_json['items']\n",
    "        for item in items:\n",
    "            title = item['title']\n",
    "            link = item['link']\n",
    "            \n",
    "            print(f\"Blog Title: {title}\")\n",
    "\n",
    "            # 블로그 내용을 가져와서 감정 분석\n",
    "            sentiment = analyze_emot(link)\n",
    "            if sentiment == 'positive':\n",
    "                social_media_mention[1] += 1\n",
    "            elif sentiment == 'negative':\n",
    "                social_media_mention[2] += 1\n",
    "            elif sentiment == 'neutral':\n",
    "                social_media_mention[3] += 1\n",
    "            print(social_media_mention)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred while fetching search results for model {model}: {e}\")\n",
    "    \n",
    "    social_media_mentions.append(social_media_mention)\n",
    "\n",
    "print(social_media_mentions)\n",
    "'''\n",
    "# 결과를 DataFrame으로 변환하여 CSV 파일로 저장\n",
    "df_results = pd.DataFrame(social_media_mentions, columns=['Model', 'Positive', 'Negative', 'Neutral'])\n",
    "df_results.to_csv('sentiment_analysis_results.csv', index=False, encoding='utf-8')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5111e41-56f1-492a-8048-d8cc424bac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "result_df = pd.DataFrame(social_media_mentions, columns=['model', 'positive', 'negative', 'neutral'])\n",
    "result_df.to_csv('./SOCIAL_MEDIA_MENTIONS.csv', mode='a', header=False, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c79cd862-d8f5-446a-871e-ab18ba54bfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                model  positive  negative  \\\n",
      "0                       나이키 P-6000 프리미엄 한글날 포톤 더스트 카키        21         0   \n",
      "1                       살로몬 RX 슬라이드 3.0 아몬드 밀크 알로에 워시        23         0   \n",
      "2                                    아디다스 슈퍼스타 트리플 블랙        49         3   \n",
      "3                            나이키 에어포스 1 미드 '07 WB 플랙스        46         5   \n",
      "4                      조던 1 x 트래비스 스캇 레트로 하이 OG SP 모카        43         3   \n",
      "5                            조던 1 레트로 하이 스파이더맨 넥스트 챕터        41         3   \n",
      "6           (W) 버켄스탁 보스턴 소프트 풋베드 스웨이드 레더 핑크 클레이 - 내로우        50         3   \n",
      "7                                 아디다스 아딜렛 22 그레이 파이브        35         6   \n",
      "8                          (W) 헌터 플레이 쇼트 레인 부츠 징크 그레이        64         4   \n",
      "9                                     조던 1 로우 OG 스타피쉬        63         4   \n",
      "10                    나이키 줌X 인빈시블 런 플라이니트 3 화이트 포톤더스트        55         3   \n",
      "11  우포스 x 타카히로미야시타 더 솔로이스트 더 데이 더 홀 월드 웬 어웨이 플립-플랍 블랙         9         0   \n",
      "12                            아디다스 x 아트모스 아디매틱 화이트 블랙        21         2   \n",
      "\n",
      "    neutral  \n",
      "0        18  \n",
      "1        65  \n",
      "2        33  \n",
      "3        42  \n",
      "4        53  \n",
      "5        54  \n",
      "6        33  \n",
      "7        28  \n",
      "8        32  \n",
      "9        31  \n",
      "10       34  \n",
      "11       12  \n",
      "12       30  \n"
     ]
    }
   ],
   "source": [
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1fff3a2-53e7-43f9-babe-6a753c4b6cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74281c61-ed82-4a8d-86e3-3b7ff7063766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114    온 러닝 x 포스트 아카이브 팩션 (파프) 클라우드몬스터 2 문더스트 초크\n",
      "115                          (W) 호카 호파라 2 쉬프팅 샌드\n",
      "116                 (W) 나이키 P-6000 코코넛 밀크 메탈릭 실버\n",
      "117                    나이키 줌X 스트릭플라이 화이트 클리어 제이드\n",
      "118                 아식스 x 언어펙티드 젤 카야노 14 갤럭시 화이트\n",
      "Name: model, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(model_list[57:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6043dab0-9529-4a22-9483-3626d48babdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(model_list[57:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a0d0a6-cd08-4395-8302-49d2f322ebe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57                 나이키 P-6000 프리미엄 한글날 포톤 더스트 카키\n",
      "58                 살로몬 RX 슬라이드 3.0 아몬드 밀크 알로에 워시\n",
      "59                              아디다스 슈퍼스타 트리플 블랙\n",
      "60                      나이키 에어포스 1 미드 '07 WB 플랙스\n",
      "61                조던 1 x 트래비스 스캇 레트로 하이 OG SP 모카\n",
      "                         ...                    \n",
      "114    온 러닝 x 포스트 아카이브 팩션 (파프) 클라우드몬스터 2 문더스트 초크\n",
      "115                          (W) 호카 호파라 2 쉬프팅 샌드\n",
      "116                 (W) 나이키 P-6000 코코넛 밀크 메탈릭 실버\n",
      "117                    나이키 줌X 스트릭플라이 화이트 클리어 제이드\n",
      "118                 아식스 x 언어펙티드 젤 카야노 14 갤럭시 화이트\n",
      "Name: model, Length: 62, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6759814-8e29-4f7d-b9ef-13cb7d158969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for model in model_list :\n",
    "    i += 1\n",
    "    if model == '푸마 스피드캣 OG 스파르코 블랙 화이트':\n",
    "        print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6e9f5-f06d-4351-a072-597addb1d577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
